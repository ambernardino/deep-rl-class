{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os import getenv\n",
    "from platform import system\n",
    "\n",
    "if system() == \"Darwin\":\n",
    "    %env PATH={getenv(\"PATH\") + \":/usr/X11/bin/\"}\n",
    "\n",
    "current_jupyter_path = %pwd # type: ignore\n",
    "if not current_jupyter_path.endswith(\"deep-rl-class\"):\n",
    "    %cd ..\n",
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import the packages ðŸ“¦\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pickle\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from colabgymrender.recorder import Recorder\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, X_EPISODES, window_func\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "from logic.callbacks import TqdmCallback, SaveOnBestTrainingRewardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7vOFlpA_ONz"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "observation = env.reset()\n",
    "\n",
    "actions = []\n",
    "for _ in range(5):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  actions.append(action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, done and info\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  \n",
    "  # If the game is done (in our case we land, crashed or timeout)\n",
    "  if done:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation = env.reset()\n",
    "\n",
    "print(\"Actions taken:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs\n",
    "\n",
    "https://www.gymlibrary.ml/environments/box2d/lunar_lander/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, each value is a different information about the lander:\n",
    "- Horizontal pad coordinate (x)\n",
    "- Vertical pad coordinate (y)\n",
    "- Horizontal speed (x)\n",
    "- Vertical speed (y)\n",
    "- Angle\n",
    "- Angular speed\n",
    "- If the left leg has contact point touched the land\n",
    "- If the right leg has contact point touched the land\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "- More envs, more diverse experiences during the training\n",
    "- Use SubprocVecEnv if processor has more than 8 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"data\", \"monitor\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_envs = cpu_count() # 1\n",
    "if n_envs == 1:\n",
    "    env = Monitor(gym.make(env_name), log_dir.as_posix())\n",
    "else:\n",
    "    # use SubprocVecEnv only for high cpu core count\n",
    "    env = make_vec_env(env_name, n_envs=n_envs, vec_env_cls=SubprocVecEnv, monitor_dir=log_dir.as_posix())\n",
    "\n",
    "print(\"Using {} envs\".format(n_envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "best_model_path = Path(\"data\", \"models\", \"303_14.pkl\")\n",
    "if best_model_path.exists():\n",
    "    print(\"Loading best model\")\n",
    "    with open(best_model_path, \"rb\") as f:\n",
    "        model_params = pickle.loads(f.read())\n",
    "    model.set_parameters(model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = model.n_steps * 10\n",
    "c1 = TqdmCallback(n_envs=n_envs)\n",
    "callbacks = [c1]\n",
    "callbacks.append(SaveOnBestTrainingRewardCallback(check_freq=model.n_steps*20, env_name=env_name, verbose=0))\n",
    "results = model.learn(total_timesteps=total_timesteps, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "data_frame = load_results(log_dir)\n",
    "\n",
    "data_frame = data_frame[data_frame.l.cumsum() <= total_timesteps]\n",
    "data_frames.append(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size= 100 #total_timesteps//1000\n",
    "xy_list = [ts2xy(data_frame, X_EPISODES) for data_frame in data_frames]\n",
    "\n",
    "plt.figure(env_name, figsize=(8, 2))\n",
    "max_x = max(xy[0][-1] for xy in xy_list)\n",
    "min_x = 0\n",
    "for (_, (x, y)) in enumerate(xy_list):\n",
    "    plt.scatter(x, y, s=2)\n",
    "    if x.shape[0] >= rolling_window_size:\n",
    "        x, y_mean = window_func(x, y, rolling_window_size, numpy.mean)\n",
    "        plt.plot(x, y_mean, \"r\")\n",
    "plt.xlim(min_x, max_x)\n",
    "plt.title(env_name)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "When you evaluate your agent, you should not use your training environment but create an evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(env_name)\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"data/models/{}_{}.bin\".format(int(mean_reward), int(std_reward)))\n",
    "\n",
    "with open(Path(\"data\", \"models\", \"{}_{}.pkl\".format(int(mean_reward), int(std_reward))), \"wb\") as f:\n",
    "    f.write(pickle.dumps(model.get_parameters()))\n",
    "print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records a test run\n",
    "\n",
    "If this triggers an error, please make sure that you launched XQuartz on the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_test_run = False\n",
    "\n",
    "# avoids creating multiple screens\n",
    "if type(env).__name__ != \"Recorder\":\n",
    "    env = gym.make(env_name)\n",
    "    directory = Path(\"data\", \"video\")\n",
    "    env = Recorder(env, directory.as_posix())\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "# This exports makes sure that the video is encoded properly (that is why is take more time)\n",
    "if export_test_run:\n",
    "    env.play()\n",
    "    !mv \"__temp__.mp4\" \"data/video/test_run_recording.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a857566195ce20e3e7ac6cd7ad7d4f6e93b50a15eb21274247fe0685339f7b5b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
