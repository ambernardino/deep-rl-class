{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os import getenv\n",
    "%env PATH={getenv(\"PATH\") + \":/usr/X11/bin/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import the packages üì¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from colabgymrender.recorder import Recorder\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7vOFlpA_ONz"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "observation = env.reset()\n",
    "\n",
    "actions = []\n",
    "for _ in range(5):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  actions.append(action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, done and info\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  \n",
    "  # If the game is done (in our case we land, crashed or timeout)\n",
    "  if done:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation = env.reset()\n",
    "\n",
    "print(\"Actions taken:\", actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs\n",
    "\n",
    "https://www.gymlibrary.ml/environments/box2d/lunar_lander/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, each value is a different information about the lander:\n",
    "- Horizontal pad coordinate (x)\n",
    "- Vertical pad coordinate (y)\n",
    "- Horizontal speed (x)\n",
    "- Vertical speed (y)\n",
    "- Angle\n",
    "- Angular speed\n",
    "- If the left leg has contact point touched the land\n",
    "- If the right leg has contact point touched the land\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Environment\n",
    "- We create a vectorized environment (method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env(env_name, n_envs=16)\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.progress_bar = None\n",
    "    \n",
    "    def _on_training_start(self):\n",
    "        self.progress_bar = tqdm(total=self.locals['total_timesteps'])\n",
    "    \n",
    "    def _on_step(self):\n",
    "        self.progress_bar.update(1)\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.progress_bar.close()\n",
    "        self.progress_bar = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "with open(Path('data', 'models', \"263_44.pkl\"), \"rb\") as f:\n",
    "    model_params = pickle.loads(f.read())\n",
    "model.set_parameters(model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train the PPO agent üèÉ\n",
    "- Let's train our agent for 500,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~10min, but you can use less timesteps if you just want to try it out.\n",
    "- During the training, take a ‚òï break you deserved it ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=1000000)\n",
    "c = TqdmCallback()\n",
    "model.learn(total_timesteps=5000, callback=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the agent üìà\n",
    "\n",
    "When you evaluate your agent, you should not use your training environment but create an evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(env_name)\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "\n",
    "#target: `200.20 +/- 20.80` after training for 1 million steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"data/models/{}_{}.bin\".format(int(mean_reward), int(std_reward)))\n",
    "\n",
    "with open(Path(\"data\", \"models\", \"{}_{}.pkl\".format(int(mean_reward), int(std_reward))), \"wb\") as f:\n",
    "    f.write(pickle.dumps(model.get_parameters()))\n",
    "print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoids creating multiple screens\n",
    "if type(env).__name__ != \"Recorder\":\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    directory = \"data/video\"\n",
    "    env = Recorder(env, directory)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "# env.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a857566195ce20e3e7ac6cd7ad7d4f6e93b50a15eb21274247fe0685339f7b5b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('deep-rl-class-xwJls31r')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
